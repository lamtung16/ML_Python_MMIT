- give code python
- library: numpy, no use of sklearn
- mechanism: regression decision tree, but there are 3 differences
first: data: input has n features, target is not a singular point, it is an interval (y_target_low, y_target_up) where y_low can be -inf and y_up can be inf
second: Splitting Criterion: not MSE, it is Hinge Error, the formula is loss_value(y_pred, (y_target_low, y_target_up)) = (relu(y_target_low - y_pred + e))^p + (relu(y_pred - y_target_up + e))^p, e is the margin_length (a hyperparameter) and p is either 1 or 2 (a hyperparameter)
third: Prediction: At a leaf node, there are m training samples, so there are 2*m ends values (y_target_low_i, y_target_up_i) where i from 1 to m. Consider all 2*m values except y_target_low=-inf and y_target_up=inf, the predicted value is one of them.  the predicted value minimizes the Hinge Error at that node.
- everything else is the same as regression decision tree in sklearn
- it's a class similar to "from sklearn import tree"
- given an example: 4 instances (X,y) where X = [[1,1],[2,1],[3,1],[4,1]] and y = [[-inf,1],[1,2],[3,4],[4,inf]], splitting one time, predicted values is expected 1 and 4